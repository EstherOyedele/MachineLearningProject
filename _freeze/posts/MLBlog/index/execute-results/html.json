{
  "hash": "665d7fba3233a7d0d89485021a325924",
  "result": {
    "markdown": "---\ntitle: \"Fundamentals of Machine Learning\"\n---\n\n\n# Probability Theory and Random Variables\n\n## Introduction\n\nProbability theory begins with the fundamental concepts of events and sample space. These concepts lay the groundwork for understanding the likelihood of different outcomes in a given scenario. In this section, we'll explore these basic probability concepts and how they form the building blocks of more advanced probability theory.\n\n## Basic Probability Concepts\n\n**Events:** In probability theory, an event is an outcome or a set of outcomes of a random experiment. Events are the events we are interested in, and they can range from simple outcomes to complex combinations.\n\n**Sample Space:** The sample space is the set of all possible outcomes of a random experiment. It encompasses every possible scenario, providing a comprehensive view of the potential results.\n\n**Example 1:** Consider rolling a six-sided die.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Events and Sample Space Example\n\n# Define the sample space for rolling a six-sided die\nsample_space <- 1:6\n\n# Define the event \"rolling an even number\"\nevent_A <- c(2, 4, 6)\n\n# Define the event \"rolling a number greater than 4\"\nevent_B <- c(5, 6)\n\n# Display the events and sample space\ncat(\"Event A (Rolling an even number):\", event_A, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEvent A (Rolling an even number): 2 4 6 \n```\n:::\n\n```{.r .cell-code}\ncat(\"Event B (Rolling a number greater than 4):\", event_B, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEvent B (Rolling a number greater than 4): 5 6 \n```\n:::\n\n```{.r .cell-code}\ncat(\"Sample Space:\", sample_space, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSample Space: 1 2 3 4 5 6 \n```\n:::\n:::\n\n\n**Example 2:** Consider flipping a coin with Head and Tail\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample Space Example 2\n\n# Define the sample space for flipping a coin\ncoin_sample_space <- c(\"Heads\", \"Tails\")\n\n# Display the sample space for flipping a coin\ncat(\"Sample Space for Flipping a Coin:\", coin_sample_space, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSample Space for Flipping a Coin: Heads Tails \n```\n:::\n:::\n\n\n## Probability of an Event\n\nThe probability of an event is a measure of the likelihood that the event will occur. It is expressed as a value between 0 and 1, where 0 indicates impossibility, 1 indicates certainty, and values in between represent varying degrees of likelihood.\n\n#### Independence and Mutually Exclusive Events\n\n**Independence:** Events A and B are independent if the occurrence of one does not affect the occurrence of the other.\n\n**Mutually Exclusive (Disjoint):** Events A and B are mutually exclusive if they cannot both occur at the same time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Probability of an Event Example\n\n# Number of favorable outcomes for event A\nfavorable_outcomes_A <- length(event_A)\n\n# Total number of possible outcomes\ntotal_outcomes <- length(sample_space)\n\n# Calculate the probability of event A\nprobability_A <- favorable_outcomes_A / total_outcomes\n\n# Display the probability of event A\ncat(\"Probability of Event A (Rolling an even number):\", probability_A, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProbability of Event A (Rolling an even number): 0.5 \n```\n:::\n:::\n\n\n## Random Variables **and Probability Distributions**\n\nIn probability theory, a **random variable** is a variable whose possible values are outcomes of a random phenomenon. Understanding random variables is crucial in modeling uncertainty and variability in real-world scenarios. Each random variable is associated with a **probability distribution**, which describes the likelihood of different outcomes.\n\n#### **Types of Random Variables**\n\n**Discrete Random Variables**\n\n-   Examples: Number of heads in coin flips, count of customers arriving at a store.\n\n-   Rationale: Takes on a countable number of distinct values.\n\n**Continuous Random Variables**\n\n-   Examples: Height of individuals, temperature, stock prices.\n\n-   Rationale: Takes on an uncountable infinite number of possible values.\n\n#### **Probability Distributions**\n\n**Probability Mass Function (PMF)**\n\n1.  For discrete random variables.\n\n2.  Gives the probability of each possible outcome.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example PMF for a six-sided die\noutcomes <- 1:6\nprobabilities <- rep(1/6, 6)\npmf <- data.frame(Outcome = outcomes, Probability = probabilities)\n\n# Display the pmf\nprint(pmf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Outcome Probability\n1       1   0.1666667\n2       2   0.1666667\n3       3   0.1666667\n4       4   0.1666667\n5       5   0.1666667\n6       6   0.1666667\n```\n:::\n:::\n\n\n**Probability Density Function (PDF)**\n\n1.  For continuous random variables.\n\n2.  Describes the likelihood of a range of values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example PDF for a standard normal distribution\nx <- seq(-3, 3, length.out = 100)\npdf <- data.frame(X = x, Probability = dnorm(x))\n\n# Display the pdf\nprint(pdf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              X Probability\n1   -3.00000000 0.004431848\n2   -2.93939394 0.005305788\n3   -2.87878788 0.006328776\n4   -2.81818182 0.007521325\n5   -2.75757576 0.008905818\n6   -2.69696970 0.010506499\n7   -2.63636364 0.012349433\n8   -2.57575758 0.014462415\n9   -2.51515152 0.016874830\n10  -2.45454545 0.019617461\n11  -2.39393939 0.022722232\n12  -2.33333333 0.026221889\n13  -2.27272727 0.030149614\n14  -2.21212121 0.034538568\n15  -2.15151515 0.039421369\n16  -2.09090909 0.044829497\n17  -2.03030303 0.050792644\n18  -1.96969697 0.057338005\n19  -1.90909091 0.064489518\n20  -1.84848485 0.072267075\n21  -1.78787879 0.080685709\n22  -1.72727273 0.089754773\n23  -1.66666667 0.099477139\n24  -1.60606061 0.109848419\n25  -1.54545455 0.120856256\n26  -1.48484848 0.132479675\n27  -1.42424242 0.144688550\n28  -1.36363636 0.157443188\n29  -1.30303030 0.170694048\n30  -1.24242424 0.184381641\n31  -1.18181818 0.198436597\n32  -1.12121212 0.212779929\n33  -1.06060606 0.227323506\n34  -1.00000000 0.241970725\n35  -0.93939394 0.256617400\n36  -0.87878788 0.271152848\n37  -0.81818182 0.285461167\n38  -0.75757576 0.299422683\n39  -0.69696970 0.312915556\n40  -0.63636364 0.325817499\n41  -0.57575758 0.338007591\n42  -0.51515152 0.349368138\n43  -0.45454545 0.359786558\n44  -0.39393939 0.369157220\n45  -0.33333333 0.377383228\n46  -0.27272727 0.384378084\n47  -0.21212121 0.390067204\n48  -0.15151515 0.394389234\n49  -0.09090909 0.397297160\n50  -0.03030303 0.398759153\n51   0.03030303 0.398759153\n52   0.09090909 0.397297160\n53   0.15151515 0.394389234\n54   0.21212121 0.390067204\n55   0.27272727 0.384378084\n56   0.33333333 0.377383228\n57   0.39393939 0.369157220\n58   0.45454545 0.359786558\n59   0.51515152 0.349368138\n60   0.57575758 0.338007591\n61   0.63636364 0.325817499\n62   0.69696970 0.312915556\n63   0.75757576 0.299422683\n64   0.81818182 0.285461167\n65   0.87878788 0.271152848\n66   0.93939394 0.256617400\n67   1.00000000 0.241970725\n68   1.06060606 0.227323506\n69   1.12121212 0.212779929\n70   1.18181818 0.198436597\n71   1.24242424 0.184381641\n72   1.30303030 0.170694048\n73   1.36363636 0.157443188\n74   1.42424242 0.144688550\n75   1.48484848 0.132479675\n76   1.54545455 0.120856256\n77   1.60606061 0.109848419\n78   1.66666667 0.099477139\n79   1.72727273 0.089754773\n80   1.78787879 0.080685709\n81   1.84848485 0.072267075\n82   1.90909091 0.064489518\n83   1.96969697 0.057338005\n84   2.03030303 0.050792644\n85   2.09090909 0.044829497\n86   2.15151515 0.039421369\n87   2.21212121 0.034538568\n88   2.27272727 0.030149614\n89   2.33333333 0.026221889\n90   2.39393939 0.022722232\n91   2.45454545 0.019617461\n92   2.51515152 0.016874830\n93   2.57575758 0.014462415\n94   2.63636364 0.012349433\n95   2.69696970 0.010506499\n96   2.75757576 0.008905818\n97   2.81818182 0.007521325\n98   2.87878788 0.006328776\n99   2.93939394 0.005305788\n100  3.00000000 0.004431848\n```\n:::\n:::\n\n\n#### **Expectation and Variance**\n\n-   **Expectation (Mean):** Represents the average value of a random variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example calculation of expectation for a discrete random variable\nmean_discrete <- sum(outcomes * probabilities)\n\n# Display the mean\ncat(\"mean:\", mean_discrete, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmean: 3.5 \n```\n:::\n:::\n\n\n-   **Variance:** Measures the spread or dispersion of a random variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example calculation of variance for a discrete random variable\nvariance_discrete <- sum((outcomes - mean_discrete)^2 * probabilities)\n\n# Display the variance\ncat(\"variance:\", variance_discrete, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nvariance: 2.916667 \n```\n:::\n:::\n\n\n## Real-World Application\n\nUnderstanding basic probability concepts is crucial in various real-world scenarios, from predicting the weather to making informed decisions in business and finance. By grasping these fundamentals, we can quantify uncertainty and make more informed choices. Random variables and probability distributions are essential in fields like finance (modeling stock prices), healthcare (analyzing patient outcomes), and many others. These concepts form the backbone of statistical modeling and machine learning algorithms.\n\n## Conclusion\n\nProbability theory provides a solid foundation for understanding uncertainty and variability in data---a cornerstone for anyone venturing into machine learning. Mastering basic probability concepts sets the stage for a deeper exploration of probability theory. In the next sections, we'll apply these concepts through practical examples and code implementations.\n\n# **Clustering**\n\n## Introduction\n\nClustering is a technique in machine learning that involves grouping similar data points together based on certain characteristics. The goal is to uncover inherent structures within the data, making it a form of unsupervised learning. In this section, we'll explore the basics of clustering, with a focus on one popular algorithm: K-Means.\n\n## K-Means Clustering Algorithm\n\nK-Means is a partitioning method that divides a dataset into $k$ distinct, non-overlapping subsets (clusters).\n\n#### **How it Works**\n\n1.  **Initialization:** Randomly select $k$ data points as initial cluster centroids.\n\n2.  **Assignment:** Assign each data point to the cluster whose centroid is closest.\n\n3.  **Update:** Recalculate the cluster centroids based on the assigned points.\n\n4.  **Repeat:** Iterate steps 2 and 3 until convergence (minimal change in cluster assignments).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# K-Means clustering example using built-in dataset iris\ndata <- iris[, 1:4]  # Selecting relevant features\nkmeans_result <- kmeans(data, centers = 3)  # Clustering into 3 groups\n\n# Display cluster assignments\ncat(\"Cluster Assignments:\", kmeans_result$cluster, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCluster Assignments: 2 3 3 3 2 2 2 2 3 3 2 2 3 3 2 2 2 2 2 2 2 2 2 2 3 3 2 2 2 3 3 2 2 2 3 2 2 2 3 2 2 3 3 2 2 3 2 3 2 2 1 1 1 1 1 1 1 3 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n```\n:::\n\n```{.r .cell-code}\n# Display cluster centroids\ncat(\"Cluster Centroids:\\n\", kmeans_result$centers, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCluster Centroids:\n 6.314583 5.175758 4.738095 2.895833 3.624242 2.904762 4.973958 1.472727 1.790476 1.703125 0.2727273 0.352381 \n```\n:::\n\n```{.r .cell-code}\n# Scatter plot with cluster assignments\nplot(data[, c(1, 2)], col = kmeans_result$cluster, main = \"K-Means Clustering\", \n     xlab = \"Sepal Length\", ylab = \"Sepal Width\")\n\n# Plotting cluster centroids\npoints(kmeans_result$centers[, c(1, 2)], col = 1:3, pch = 8, cex = 2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n## Real-World Application\n\nClustering is widely used in various domains:\n\n-   **Customer Segmentation:** Identifying groups of customers with similar purchasing behaviors.\n\n-   **Image Segmentation:** Grouping pixels with similar attributes in images.\n\n-   **Anomaly Detection:** Identifying unusual patterns in data by treating normal behavior as clusters.\n\n## Conclusion\n\nUnderstanding clustering algorithms like K-Means opens up opportunities to uncover patterns in data, enabling more informed decision-making. In the next sections, we'll explore additional machine learning concepts, including regression, classification, and anomaly detection.\n\n# **Regression**\n\n## Introduction\n\nRegression is a fundamental concept in machine learning that involves modeling the relationship between a dependent variable and one or more independent variables. In this section, we'll explore linear regression, a widely used regression technique, and nonlinear regression. Nonlinear regression is a powerful technique used in machine learning when the relationship between the dependent variable and the independent variables cannot be adequately modeled with a linear equation.\n\n## Linear Regression\n\nLinear regression aims to find the best-fit linear relationship between a dependent variable ($Y$) and one or more independent variables ($X$). The goal is to model the underlying pattern in the data, making it a valuable tool for prediction and understanding the relationships between variables.\n\n#### Linear Regression Model\n\n**Model Equation** $Y=β0​+β1​X1​+β2​X2​+…+βn​Xn​+ε$\n\n-   $Y$ is the dependent variable.\n\n-   $X1​,X2​,…,Xn​$ are the independent variables.\n\n-   $β0​,β1​,…,βn​$ are the model coefficients.\n\n-   $ε$ is the error term.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Linear regression example using built-in dataset mtcars\nlinear_model <- lm(mpg ~ hp, data = mtcars)  # Predicting mpg based on horsepower\n\n# Display regression summary\nsummary(linear_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = mpg ~ hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7121 -2.1122 -0.8854  1.5819  8.2360 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 30.09886    1.63392  18.421  < 2e-16 ***\nhp          -0.06823    0.01012  -6.742 1.79e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 30 degrees of freedom\nMultiple R-squared:  0.6024,\tAdjusted R-squared:  0.5892 \nF-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07\n```\n:::\n\n```{.r .cell-code}\n# Scatter plot with linear regression line\nplot(mtcars$hp, mtcars$mpg, main = \"Linear Regression\", xlab = \"Horsepower\", ylab = \"Miles Per Gallon\")\n\n# Adding regression line\nabline(linear_model, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n## Nonlinear Regression Model\n\nWhile linear regression assumes a linear relationship between variables, nonlinear regression allows for more complex relationships. The model equation is a nonlinear function of the independent variables, providing greater flexibility in capturing intricate patterns in the data.\n\n#### Nonlinear Regression Model\n\n**Model Equation**\n\n$Y=β0​+β1​f1​(X1​)+β2​f2​(X2​)+…+βn​fn​(Xn​)+ε$\n\n-   $Y$ is the dependent variable.\n\n-   $f1​(X1​),f2​(X2​),…,fn​(Xn​)$ are nonlinear functions of the independent variables.\n\n-   $β0​,β1​,…,βn​$ are the model coefficients.\n\n-   $ε$ is the error term.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Nonlinear regression example using built-in dataset mtcars\nnonlinear_model <- nls(mpg ~ a * hp^b, data = mtcars, start = list(a = 1, b = 1))  # Nonlinear model: a * hp^b\n\n# Display regression summary\nsummary(nonlinear_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFormula: mpg ~ a * hp^b\n\nParameters:\n   Estimate Std. Error t value Pr(>|t|)    \na 272.11686   74.03677   3.675 0.000924 ***\nb  -0.54043    0.05826  -9.277 2.55e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.102 on 30 degrees of freedom\n\nNumber of iterations to convergence: 18 \nAchieved convergence tolerance: 5.914e-07\n```\n:::\n\n```{.r .cell-code}\n# Scatter plot with nonlinear regression curve\nplot(mtcars$hp, mtcars$mpg, main = \"Nonlinear Regression\", xlab = \"Horsepower\", ylab = \"Miles Per Gallon\")\n\n# Adding nonlinear regression curve\nlines(mtcars$hp, predict(nonlinear_model), col = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## **Real-World Applications**\n\nLinear regression finds applications in various fields:\n\n-   **Economics:** Predicting economic indicators based on various factors.\n\n-   **Healthcare:** Modeling the relationship between patient characteristics and health outcomes.\n\n-   **Finance:** Predicting stock prices based on historical data.\n\nNonlinear regression is applicable in scenarios where relationships are better represented by curves or other nonlinear patterns:\n\n-   **Pharmacokinetics:** Modeling drug concentration over time in the body.\n\n-   **Biology:** Modeling population growth, enzyme kinetics, etc.\n\n-   **Physics:** Modeling complex physical phenomena.\n\n## **Conclusion**\n\nUnderstanding linear regression allows us to make predictions and gain insights into the relationships between variables. Nonlinear regression expands our modeling capabilities, allowing us to capture more intricate relationships in the data. In the upcoming sections, we'll explore classification, and anomaly detection.\n\n# **Classification**\n\n## **Introduction**\n\nClassification is a type of supervised machine learning where the goal is to predict the categorical class labels of new instances based on past observations. Classification involves training a model to learn the mapping between input features and predefined categories or classes. The trained model can then be used to predict the class labels of new, unseen instances. In this section, we'll explore the basics of classification, with a focus on one of the most widely used algorithms: Decision Trees.\n\n## **Decision Trees for Classification**\n\nA decision tree is a tree-like model where each internal node represents a decision based on a feature, each branch represents the outcome of the decision, and each leaf node represents the class label.\n\n#### **How it Works**\n\n1.  **Feature Selection**: Choose the feature that best splits the data into classes (based on criteria like Gini impurity or information gain).\n\n2.  **Splitting:** Create a branch for each possible value of the selected feature.\n\n3.  **Repeat:** Recursively apply steps 1 and 2 to each subset of data until a stopping criterion is met.\n\n4.  **Leaf Nodes:** Assign a class label to each leaf node.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Decision tree example using built-in dataset iris\nlibrary(rpart)\n\n# Creating a decision tree model\ntree_model <- rpart(Species ~ ., data = iris, method = \"class\")\n\n# Displaying the decision tree plot\nplot(tree_model)\ntext(tree_model)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n## **Real-World Application**\n\nDecision trees find applications in various domains:\n\n-   **Medical Diagnosis:** Predicting diseases based on patient symptoms.\n\n-   **Customer Churn Prediction:** Identifying customers likely to churn from a service.\n\n-   **Credit Scoring:** Assessing credit risk of loan applicants.\n\n## **Conclusion**\n\nClassification is a powerful tool for making predictions and categorizing data into distinct classes. In the last sections, we'll explore anomaly detection, a technique used to identify unusual patterns in data.\n\n# **Anomaly Detection**\n\n## Introduction\n\nAnomaly detection, also known as outlier detection, is a machine learning technique used to identify patterns in data that deviate significantly from the norm. In this section, we'll explore the basics of anomaly detection, with a focus on a common method: Z-score-based outlier detection.\n\n## **Outlier Detection Methods**\n\n**Z-Score Method**\n\n-   Calculates the Z-score for each data point, representing how many standard deviations it is from the mean.\n\n-   Points with a Z-score above a certain threshold are considered outliers.\n\n**Example:** Outlier Detection using Z-Score\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Outlier detection using Z-score\ndata <- c(2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31)\n\n# Calculate Z-scores\nz_scores <- (data - mean(data)) / sd(data)\n\n# Set Z-score threshold for outliers\nz_threshold <- 2\n\n# Identify outliers\noutliers <- abs(z_scores) > z_threshold\n\n# Display identified outliers\ncat(\"Outliers:\", data[outliers], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOutliers:  \n```\n:::\n\n```{.r .cell-code}\n# Scatter plot with highlighted outliers\nplot(1:length(data), data, pch = ifelse(outliers, 19, 1), main = \"Scatter Plot with Outliers\", xlab = \"Index\", ylab = \"Values\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n## **Real-World Application**\n\nAnomaly detection is applied in various fields:\n\n-   **Fraud Detection:** Identifying unusual patterns in financial transactions.\n\n-   **Network Security:** Detecting unusual behavior in network traffic.\n\n-   **Manufacturing Quality Control:** Identifying defective products.\n\n## **Conclusion**\n\nAnomaly detection is crucial for identifying unusual patterns that may indicate errors or potential issues.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}