[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fundamentals of Machine Learning",
    "section": "",
    "text": "No matching items\n\nCopyrightEsther Oyedele, 2023. All Rights Reserved"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Esther Oyedele, and I am currently a Doctoral student in Geosciences at Virginia Tech.\nThis blog is part of a project for my Machine Learning class, where I aim to provide a beginner-friendly exploration of the fundamentals of machine learning. Please note that this content covers the basics and includes common code snippets without delving into technical complexities. If you are looking to embark on your machine learning journey, I hope this blog serves as a helpful introduction to essential concepts."
  },
  {
    "objectID": "about.html#about-this-blog",
    "href": "about.html#about-this-blog",
    "title": "About",
    "section": "",
    "text": "My name is Esther Oyedele, and I am currently a Doctoral student in Geosciences at Virginia Tech.\nThis blog is part of a project for my Machine Learning class, where I aim to provide a beginner-friendly exploration of the fundamentals of machine learning. Please note that this content covers the basics and includes common code snippets without delving into technical complexities. If you are looking to embark on your machine learning journey, I hope this blog serves as a helpful introduction to essential concepts."
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Overview",
    "section": "",
    "text": "Machine learning often deals with uncertainty and randomness, and understanding the fundamentals of probability theory and random variables is crucial for building robust models. In this blog post, we’ll embark on a journey into the realm of basic machine learning concepts. In the each sections, we’ll apply each concepts through practical examples and code implementations using the R programming language.\n\n\n\nCopyrightEsther Oyedele, 2023. All Rights Reserved"
  },
  {
    "objectID": "Post1b.html",
    "href": "Post1b.html",
    "title": "Basic Probability Concepts",
    "section": "",
    "text": "Events: In probability theory, an event is an outcome or a set of outcomes of a random experiment. Events are the events we are interested in, and they can range from simple outcomes to complex combinations.\nSample Space: The sample space is the set of all possible outcomes of a random experiment. It encompasses every possible scenario, providing a comprehensive view of the potential results.\nExample 1: Consider rolling a six-sided die.\nCode\n# Events and Sample Space Example\n\n# Define the sample space for rolling a six-sided die\nsample_space &lt;- 1:6\n\n# Define the event \"rolling an even number\"\nevent_A &lt;- c(2, 4, 6)\n\n# Define the event \"rolling a number greater than 4\"\nevent_B &lt;- c(5, 6)\n\n# Display the events and sample space\ncat(\"Event A (Rolling an even number):\", event_A, \"\\n\")\n\n\nEvent A (Rolling an even number): 2 4 6 \n\n\nCode\ncat(\"Event B (Rolling a number greater than 4):\", event_B, \"\\n\")\n\n\nEvent B (Rolling a number greater than 4): 5 6 \n\n\nCode\ncat(\"Sample Space:\", sample_space, \"\\n\")\n\n\nSample Space: 1 2 3 4 5 6\nExample 2: Consider flipping a coin with Head and Tail\nCode\n# Sample Space Example 2\n\n# Define the sample space for flipping a coin\ncoin_sample_space &lt;- c(\"Heads\", \"Tails\")\n\n# Display the sample space for flipping a coin\ncat(\"Sample Space for Flipping a Coin:\", coin_sample_space, \"\\n\")\n\n\nSample Space for Flipping a Coin: Heads Tails"
  },
  {
    "objectID": "Post1b.html#probability-of-an-event",
    "href": "Post1b.html#probability-of-an-event",
    "title": "Basic Probability Concepts",
    "section": "Probability of an Event",
    "text": "Probability of an Event\nThe probability of an event is a measure of the likelihood that the event will occur. It is expressed as a value between 0 and 1, where 0 indicates impossibility, 1 indicates certainty, and values in between represent varying degrees of likelihood.\n\nIndependence and Mutually Exclusive Events\nIndependence: Events A and B are independent if the occurrence of one does not affect the occurrence of the other.\nMutually Exclusive (Disjoint): Events A and B are mutually exclusive if they cannot both occur at the same time.\n\n\nCode\n# Probability of an Event Example\n\n# Number of favorable outcomes for event A\nfavorable_outcomes_A &lt;- length(event_A)\n\n# Total number of possible outcomes\ntotal_outcomes &lt;- length(sample_space)\n\n# Calculate the probability of event A\nprobability_A &lt;- favorable_outcomes_A / total_outcomes\n\n# Display the probability of event A\ncat(\"Probability of Event A (Rolling an even number):\", probability_A, \"\\n\")\n\n\nProbability of Event A (Rolling an even number): 0.5 \n\n\n** The result of a coin flip using a Bernoulli distribution in R, you can create a bar plot that represents the probability of getting Heads and Tails. The bar heights will correspond to the probabilities of success (Heads) and failure (Tails).\n\n\nCode\n# Set the probability of getting Heads\np_heads &lt;- 0.5\n\n# Calculate the probability of getting Tails\np_tails &lt;- 1 - p_heads\n\n# Create a bar plot\nbarplot(c(p_heads, p_tails), names.arg = c(\"Heads\", \"Tails\"),\n        col = c(\"blue\", \"red\"), ylim = c(0, 1),\n        main = \"Coin Flip Probability\",\n        xlab = \"Outcome\", ylab = \"Probability\")\n\n# Add probability labels to the bars\ntext(1:2, c(p_heads, p_tails), labels = c(p_heads, p_tails), pos = 3, col = \"black\")\n\n\n\n\n\nExample 3: Here is an example where we use a Gaussian distribution to model continuous data, such as the distribution of heights in a population. In this case, we’ll use the ‘dnorm’ function to generate a Gaussian distribution and create a histogram to visualize it.\n\n\nCode\n# Set the parameters for the Gaussian distribution\nmean_height &lt;- 170  # mean height in centimeters\nsd_height &lt;- 10      # standard deviation of height\n\n# Generate a sample of heights from a Gaussian distribution\nnum_samples &lt;- 1000\nheights &lt;- rnorm(num_samples, mean = mean_height, sd = sd_height)\n\n# Create a histogram to visualize the Gaussian distribution\nhist(heights, breaks = 30, col = \"skyblue\", main = \"Gaussian Distribution of Heights\",\n     xlab = \"Height (cm)\", ylab = \"Frequency\")\n\n# Add a curve representing the theoretical Gaussian distribution\ncurve(dnorm(x, mean = mean_height, sd = sd_height), col = \"darkblue\", lwd = 2, add = TRUE)"
  },
  {
    "objectID": "Post1c.html",
    "href": "Post1c.html",
    "title": "Random Variables and Probability Distributions",
    "section": "",
    "text": "In probability theory, a random variable is a variable whose possible values are outcomes of a random phenomenon. Understanding random variables is crucial in modeling uncertainty and variability in real-world scenarios. Each random variable is associated with a probability distribution, which describes the likelihood of different outcomes."
  },
  {
    "objectID": "Post1d.html",
    "href": "Post1d.html",
    "title": "Real-World Application",
    "section": "",
    "text": "Understanding basic probability concepts is crucial in various real-world scenarios, from predicting the weather to making informed decisions in business and finance. By grasping these fundamentals, we can quantify uncertainty and make more informed choices. Random variables and probability distributions are essential in fields like finance (modeling stock prices), healthcare (analyzing patient outcomes), and many others. These concepts form the backbone of statistical modeling and machine learning algorithms.\n\n\n\nCopyrightEsther Oyedele, 2023. All Rights Reserved"
  },
  {
    "objectID": "Post1e.html",
    "href": "Post1e.html",
    "title": "Conclusion",
    "section": "",
    "text": "Probability theory provides a solid foundation for understanding uncertainty and variability in data—a cornerstone for anyone venturing into machine learning. Mastering basic probability concepts sets the stage for a deeper exploration of probability theory. In the next sections, we’ll apply these concepts through practical examples and code implementations.\n\n\n\nCopyrightEsther Oyedele, 2023. All Rights Reserved"
  },
  {
    "objectID": "Post2a.html",
    "href": "Post2a.html",
    "title": "Introduction",
    "section": "",
    "text": "Clustering is a technique in machine learning that involves grouping similar data points together based on certain characteristics. The goal is to uncover inherent structures within the data, making it a form of unsupervised learning. In this section, we’ll explore the basics of clustering, with a focus on one popular algorithm: K-Means.\n\n\n\nCopyrightEsther Oyedele, 2023. All Rights Reserved"
  },
  {
    "objectID": "Post2b.html",
    "href": "Post2b.html",
    "title": "K-Means Clustering Algorithm",
    "section": "",
    "text": "K-Means is a partitioning method that divides a dataset into \\(k\\) distinct, non-overlapping subsets (clusters)."
  },
  {
    "objectID": "Post2c.html",
    "href": "Post2c.html",
    "title": "Real-World Application",
    "section": "",
    "text": "Clustering is widely used in various domains:"
  },
  {
    "objectID": "Post2d.html",
    "href": "Post2d.html",
    "title": "Conclusion",
    "section": "",
    "text": "Understanding clustering algorithms like K-Means opens up opportunities to uncover patterns in data, enabling more informed decision-making. In the next sections, we’ll explore additional machine learning concepts, including regression, classification, and anomaly detection.\n\n\n\nCopyrightEsther Oyedele, 2023. All Rights Reserved"
  },
  {
    "objectID": "Post3a.html",
    "href": "Post3a.html",
    "title": "Introduction",
    "section": "",
    "text": "Regression is a fundamental concept in machine learning that involves modeling the relationship between a dependent variable and one or more independent variables. In this section, we’ll explore linear regression, a widely used regression technique, and nonlinear regression. Nonlinear regression is a powerful technique used in machine learning when the relationship between the dependent variable and the independent variables cannot be adequately modeled with a linear equation.\n\n\n\nCopyrightEsther Oyedele, 2023. All Rights Reserved"
  },
  {
    "objectID": "Post3b.html",
    "href": "Post3b.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Linear regression aims to find the best-fit linear relationship between a dependent variable (\\(Y\\)) and one or more independent variables (\\(X\\)). The goal is to model the underlying pattern in the data, making it a valuable tool for prediction and understanding the relationships between variables."
  },
  {
    "objectID": "Post3c.html",
    "href": "Post3c.html",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "While linear regression assumes a linear relationship between variables, nonlinear regression allows for more complex relationships. The model equation is a nonlinear function of the independent variables, providing greater flexibility in capturing intricate patterns in the data."
  },
  {
    "objectID": "Post3d.html",
    "href": "Post3d.html",
    "title": "Real-World Applications",
    "section": "",
    "text": "Linear regression finds applications in various fields:\n\nEconomics: Predicting economic indicators based on various factors.\nHealthcare: Modeling the relationship between patient characteristics and health outcomes.\nFinance: Predicting stock prices based on historical data.\n\nNonlinear regression is applicable in scenarios where relationships are better represented by curves or other nonlinear patterns:\n\nPharmacokinetics: Modeling drug concentration over time in the body.\nBiology: Modeling population growth, enzyme kinetics, etc.\nPhysics: Modeling complex physical phenomena.\n\n\n\n\nCopyrightEsther Oyedele, 2023. All Rights Reserved"
  },
  {
    "objectID": "Post3e.html",
    "href": "Post3e.html",
    "title": "Conclusion",
    "section": "",
    "text": "Understanding linear regression allows us to make predictions and gain insights into the relationships between variables. Nonlinear regression expands our modeling capabilities, allowing us to capture more intricate relationships in the data. In the upcoming sections, we’ll explore classification, and anomaly detection.\n\n\n\nCopyrightEsther Oyedele, 2023. All Rights Reserved"
  },
  {
    "objectID": "Post4a.html",
    "href": "Post4a.html",
    "title": "Introduction",
    "section": "",
    "text": "Classification is a type of supervised machine learning where the goal is to predict the categorical class labels of new instances based on past observations. Classification involves training a model to learn the mapping between input features and predefined categories or classes. The trained model can then be used to predict the class labels of new, unseen instances. In this section, we’ll explore the basics of classification, with a focus on one of the most widely used algorithms, Decision Trees.\n\n\n\nCopyrightEsther Oyedele, 2023. All Rights Reserved"
  },
  {
    "objectID": "Post4b.html",
    "href": "Post4b.html",
    "title": "Decision Trees for Classification",
    "section": "",
    "text": "A decision tree is a tree-like model where each internal node represents a decision based on a feature, each branch represents the outcome of the decision, and each leaf node represents the class label."
  },
  {
    "objectID": "Post4c.html",
    "href": "Post4c.html",
    "title": "Real-World Application",
    "section": "",
    "text": "Decision trees find applications in various domains:"
  },
  {
    "objectID": "Post4d.html",
    "href": "Post4d.html",
    "title": "Conclusion",
    "section": "",
    "text": "Classification is a powerful tool for making predictions and categorizing data into distinct classes. In the last sections, we’ll explore anomaly detection, a technique used to identify unusual patterns in data.\n\n\n\nCopyrightEsther Oyedele, 2023. All Rights Reserved"
  },
  {
    "objectID": "Post5a.html",
    "href": "Post5a.html",
    "title": "Introduction",
    "section": "",
    "text": "Anomaly detection, also known as outlier detection, is a machine learning technique used to identify patterns in data that deviate significantly from the norm. In this section, we’ll explore the basics of anomaly detection, with a focus on a common method: Z-score-based outlier detection.\n\n\n\nCopyrightEsther Oyedele, 2023. All Rights Reserved"
  },
  {
    "objectID": "Post5b.html",
    "href": "Post5b.html",
    "title": "Outlier Detection Methods",
    "section": "",
    "text": "Z-Score Method\n\nCalculates the Z-score for each data point, representing how many standard deviations it is from the mean.\nPoints with a Z-score above a certain threshold are considered outliers.\n\nExample: Outlier Detection using Z-Score\n\n\nCode\n# Outlier detection using Z-score\ndata &lt;- c(2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31)\n\n# Calculate Z-scores\nz_scores &lt;- (data - mean(data)) / sd(data)\n\n# Set Z-score threshold for outliers\nz_threshold &lt;- 2\n\n# Identify outliers\noutliers &lt;- abs(z_scores) &gt; z_threshold\n\n# Display identified outliers\ncat(\"Outliers:\", data[outliers], \"\\n\")\n\n\nOutliers:  \n\n\nCode\n# Scatter plot with highlighted outliers\nplot(1:length(data), data, pch = ifelse(outliers, 19, 1), main = \"Scatter Plot with Outliers\", xlab = \"Index\", ylab = \"Values\")\n\n\n\n\n\n\n\n\nCopyrightEsther Oyedele, 2023. All Rights Reserved"
  },
  {
    "objectID": "Post5c.html",
    "href": "Post5c.html",
    "title": "Real-World Application",
    "section": "",
    "text": "Anomaly detection is applied in various fields:\n\nFraud Detection: Identifying unusual patterns in financial transactions.\nNetwork Security: Detecting unusual behavior in network traffic.\nManufacturing Quality Control: Identifying defective products.\n\n\n\n\nCopyrightEsther Oyedele, 2023. All Rights Reserved"
  },
  {
    "objectID": "Post5d.html",
    "href": "Post5d.html",
    "title": "Conclusion",
    "section": "",
    "text": "Anomaly detection is crucial for identifying unusual patterns that may indicate errors or potential issues.\n\n\n\nCopyrightEsther Oyedele, 2023. All Rights Reserved"
  },
  {
    "objectID": "Post1a.html",
    "href": "Post1a.html",
    "title": "Introduction",
    "section": "",
    "text": "Machine learning often deals with uncertainty and randomness, and understanding the fundamentals of probability theory and random variables is crucial for building robust models.\nProbability theory provides a solid foundation for understanding uncertainty and variability in data—a cornerstone for anyone venturing into machine learning. Probability theory begins with the fundamental concepts of events and sample space. These concepts lay the groundwork for understanding the likelihood of different outcomes in a given scenario.\nIn this section, we’ll explore these basic probability concepts and how they form the building blocks of more advanced probability theory, and delve into the concept of random variables.\n\n\n\nCopyrightEsther Oyedele, 2023. All Rights Reserved"
  },
  {
    "objectID": "posts/MLBlog/backup.html",
    "href": "posts/MLBlog/backup.html",
    "title": "Fundamentals of Machine Learning",
    "section": "",
    "text": "Probability theory begins with the fundamental concepts of events and sample space. These concepts lay the groundwork for understanding the likelihood of different outcomes in a given scenario. In this section, we’ll explore these basic probability concepts and how they form the building blocks of more advanced probability theory.\n\n\n\nEvents: In probability theory, an event is an outcome or a set of outcomes of a random experiment. Events are the events we are interested in, and they can range from simple outcomes to complex combinations.\nSample Space: The sample space is the set of all possible outcomes of a random experiment. It encompasses every possible scenario, providing a comprehensive view of the potential results.\nExample 1: Consider rolling a six-sided die.\n\n# Events and Sample Space Example\n\n# Define the sample space for rolling a six-sided die\nsample_space &lt;- 1:6\n\n# Define the event \"rolling an even number\"\nevent_A &lt;- c(2, 4, 6)\n\n# Define the event \"rolling a number greater than 4\"\nevent_B &lt;- c(5, 6)\n\n# Display the events and sample space\ncat(\"Event A (Rolling an even number):\", event_A, \"\\n\")\n\nEvent A (Rolling an even number): 2 4 6 \n\ncat(\"Event B (Rolling a number greater than 4):\", event_B, \"\\n\")\n\nEvent B (Rolling a number greater than 4): 5 6 \n\ncat(\"Sample Space:\", sample_space, \"\\n\")\n\nSample Space: 1 2 3 4 5 6 \n\n\nExample 2: Consider flipping a coin with Head and Tail\n\n# Sample Space Example 2\n\n# Define the sample space for flipping a coin\ncoin_sample_space &lt;- c(\"Heads\", \"Tails\")\n\n# Display the sample space for flipping a coin\ncat(\"Sample Space for Flipping a Coin:\", coin_sample_space, \"\\n\")\n\nSample Space for Flipping a Coin: Heads Tails \n\n\n\n\n\nThe probability of an event is a measure of the likelihood that the event will occur. It is expressed as a value between 0 and 1, where 0 indicates impossibility, 1 indicates certainty, and values in between represent varying degrees of likelihood.\n\n\nIndependence: Events A and B are independent if the occurrence of one does not affect the occurrence of the other.\nMutually Exclusive (Disjoint): Events A and B are mutually exclusive if they cannot both occur at the same time.\n\n# Probability of an Event Example\n\n# Number of favorable outcomes for event A\nfavorable_outcomes_A &lt;- length(event_A)\n\n# Total number of possible outcomes\ntotal_outcomes &lt;- length(sample_space)\n\n# Calculate the probability of event A\nprobability_A &lt;- favorable_outcomes_A / total_outcomes\n\n# Display the probability of event A\ncat(\"Probability of Event A (Rolling an even number):\", probability_A, \"\\n\")\n\nProbability of Event A (Rolling an even number): 0.5 \n\n\n\n\n\n\nIn probability theory, a random variable is a variable whose possible values are outcomes of a random phenomenon. Understanding random variables is crucial in modeling uncertainty and variability in real-world scenarios. Each random variable is associated with a probability distribution, which describes the likelihood of different outcomes.\n\n\nDiscrete Random Variables\n\nExamples: Number of heads in coin flips, count of customers arriving at a store.\nRationale: Takes on a countable number of distinct values.\n\nContinuous Random Variables\n\nExamples: Height of individuals, temperature, stock prices.\nRationale: Takes on an uncountable infinite number of possible values.\n\n\n\n\nProbability Mass Function (PMF)\n\nFor discrete random variables.\nGives the probability of each possible outcome.\n\n\n# Example PMF for a six-sided die\noutcomes &lt;- 1:6\nprobabilities &lt;- rep(1/6, 6)\npmf &lt;- data.frame(Outcome = outcomes, Probability = probabilities)\n\n# Display the pmf\nprint(pmf)\n\n  Outcome Probability\n1       1   0.1666667\n2       2   0.1666667\n3       3   0.1666667\n4       4   0.1666667\n5       5   0.1666667\n6       6   0.1666667\n\n\nProbability Density Function (PDF)\n\nFor continuous random variables.\nDescribes the likelihood of a range of values.\n\n\n# Example PDF for a standard normal distribution\nx &lt;- seq(-3, 3, length.out = 100)\npdf &lt;- data.frame(X = x, Probability = dnorm(x))\n\n# Display the pdf\nprint(pdf)\n\n              X Probability\n1   -3.00000000 0.004431848\n2   -2.93939394 0.005305788\n3   -2.87878788 0.006328776\n4   -2.81818182 0.007521325\n5   -2.75757576 0.008905818\n6   -2.69696970 0.010506499\n7   -2.63636364 0.012349433\n8   -2.57575758 0.014462415\n9   -2.51515152 0.016874830\n10  -2.45454545 0.019617461\n11  -2.39393939 0.022722232\n12  -2.33333333 0.026221889\n13  -2.27272727 0.030149614\n14  -2.21212121 0.034538568\n15  -2.15151515 0.039421369\n16  -2.09090909 0.044829497\n17  -2.03030303 0.050792644\n18  -1.96969697 0.057338005\n19  -1.90909091 0.064489518\n20  -1.84848485 0.072267075\n21  -1.78787879 0.080685709\n22  -1.72727273 0.089754773\n23  -1.66666667 0.099477139\n24  -1.60606061 0.109848419\n25  -1.54545455 0.120856256\n26  -1.48484848 0.132479675\n27  -1.42424242 0.144688550\n28  -1.36363636 0.157443188\n29  -1.30303030 0.170694048\n30  -1.24242424 0.184381641\n31  -1.18181818 0.198436597\n32  -1.12121212 0.212779929\n33  -1.06060606 0.227323506\n34  -1.00000000 0.241970725\n35  -0.93939394 0.256617400\n36  -0.87878788 0.271152848\n37  -0.81818182 0.285461167\n38  -0.75757576 0.299422683\n39  -0.69696970 0.312915556\n40  -0.63636364 0.325817499\n41  -0.57575758 0.338007591\n42  -0.51515152 0.349368138\n43  -0.45454545 0.359786558\n44  -0.39393939 0.369157220\n45  -0.33333333 0.377383228\n46  -0.27272727 0.384378084\n47  -0.21212121 0.390067204\n48  -0.15151515 0.394389234\n49  -0.09090909 0.397297160\n50  -0.03030303 0.398759153\n51   0.03030303 0.398759153\n52   0.09090909 0.397297160\n53   0.15151515 0.394389234\n54   0.21212121 0.390067204\n55   0.27272727 0.384378084\n56   0.33333333 0.377383228\n57   0.39393939 0.369157220\n58   0.45454545 0.359786558\n59   0.51515152 0.349368138\n60   0.57575758 0.338007591\n61   0.63636364 0.325817499\n62   0.69696970 0.312915556\n63   0.75757576 0.299422683\n64   0.81818182 0.285461167\n65   0.87878788 0.271152848\n66   0.93939394 0.256617400\n67   1.00000000 0.241970725\n68   1.06060606 0.227323506\n69   1.12121212 0.212779929\n70   1.18181818 0.198436597\n71   1.24242424 0.184381641\n72   1.30303030 0.170694048\n73   1.36363636 0.157443188\n74   1.42424242 0.144688550\n75   1.48484848 0.132479675\n76   1.54545455 0.120856256\n77   1.60606061 0.109848419\n78   1.66666667 0.099477139\n79   1.72727273 0.089754773\n80   1.78787879 0.080685709\n81   1.84848485 0.072267075\n82   1.90909091 0.064489518\n83   1.96969697 0.057338005\n84   2.03030303 0.050792644\n85   2.09090909 0.044829497\n86   2.15151515 0.039421369\n87   2.21212121 0.034538568\n88   2.27272727 0.030149614\n89   2.33333333 0.026221889\n90   2.39393939 0.022722232\n91   2.45454545 0.019617461\n92   2.51515152 0.016874830\n93   2.57575758 0.014462415\n94   2.63636364 0.012349433\n95   2.69696970 0.010506499\n96   2.75757576 0.008905818\n97   2.81818182 0.007521325\n98   2.87878788 0.006328776\n99   2.93939394 0.005305788\n100  3.00000000 0.004431848\n\n\n\n\n\n\nExpectation (Mean): Represents the average value of a random variable.\n\n\n# Example calculation of expectation for a discrete random variable\nmean_discrete &lt;- sum(outcomes * probabilities)\n\n# Display the mean\ncat(\"mean:\", mean_discrete, \"\\n\")\n\nmean: 3.5 \n\n\n\nVariance: Measures the spread or dispersion of a random variable.\n\n\n# Example calculation of variance for a discrete random variable\nvariance_discrete &lt;- sum((outcomes - mean_discrete)^2 * probabilities)\n\n# Display the variance\ncat(\"variance:\", variance_discrete, \"\\n\")\n\nvariance: 2.916667 \n\n\n\n\n\n\nUnderstanding basic probability concepts is crucial in various real-world scenarios, from predicting the weather to making informed decisions in business and finance. By grasping these fundamentals, we can quantify uncertainty and make more informed choices. Random variables and probability distributions are essential in fields like finance (modeling stock prices), healthcare (analyzing patient outcomes), and many others. These concepts form the backbone of statistical modeling and machine learning algorithms.\n\n\n\nProbability theory provides a solid foundation for understanding uncertainty and variability in data—a cornerstone for anyone venturing into machine learning. Mastering basic probability concepts sets the stage for a deeper exploration of probability theory. In the next sections, we’ll apply these concepts through practical examples and code implementations."
  },
  {
    "objectID": "posts/MLBlog/backup.html#introduction",
    "href": "posts/MLBlog/backup.html#introduction",
    "title": "Fundamentals of Machine Learning",
    "section": "",
    "text": "Probability theory begins with the fundamental concepts of events and sample space. These concepts lay the groundwork for understanding the likelihood of different outcomes in a given scenario. In this section, we’ll explore these basic probability concepts and how they form the building blocks of more advanced probability theory."
  },
  {
    "objectID": "posts/MLBlog/backup.html#basic-probability-concepts",
    "href": "posts/MLBlog/backup.html#basic-probability-concepts",
    "title": "Fundamentals of Machine Learning",
    "section": "",
    "text": "Events: In probability theory, an event is an outcome or a set of outcomes of a random experiment. Events are the events we are interested in, and they can range from simple outcomes to complex combinations.\nSample Space: The sample space is the set of all possible outcomes of a random experiment. It encompasses every possible scenario, providing a comprehensive view of the potential results.\nExample 1: Consider rolling a six-sided die.\n\n# Events and Sample Space Example\n\n# Define the sample space for rolling a six-sided die\nsample_space &lt;- 1:6\n\n# Define the event \"rolling an even number\"\nevent_A &lt;- c(2, 4, 6)\n\n# Define the event \"rolling a number greater than 4\"\nevent_B &lt;- c(5, 6)\n\n# Display the events and sample space\ncat(\"Event A (Rolling an even number):\", event_A, \"\\n\")\n\nEvent A (Rolling an even number): 2 4 6 \n\ncat(\"Event B (Rolling a number greater than 4):\", event_B, \"\\n\")\n\nEvent B (Rolling a number greater than 4): 5 6 \n\ncat(\"Sample Space:\", sample_space, \"\\n\")\n\nSample Space: 1 2 3 4 5 6 \n\n\nExample 2: Consider flipping a coin with Head and Tail\n\n# Sample Space Example 2\n\n# Define the sample space for flipping a coin\ncoin_sample_space &lt;- c(\"Heads\", \"Tails\")\n\n# Display the sample space for flipping a coin\ncat(\"Sample Space for Flipping a Coin:\", coin_sample_space, \"\\n\")\n\nSample Space for Flipping a Coin: Heads Tails"
  },
  {
    "objectID": "posts/MLBlog/backup.html#probability-of-an-event",
    "href": "posts/MLBlog/backup.html#probability-of-an-event",
    "title": "Fundamentals of Machine Learning",
    "section": "",
    "text": "The probability of an event is a measure of the likelihood that the event will occur. It is expressed as a value between 0 and 1, where 0 indicates impossibility, 1 indicates certainty, and values in between represent varying degrees of likelihood.\n\n\nIndependence: Events A and B are independent if the occurrence of one does not affect the occurrence of the other.\nMutually Exclusive (Disjoint): Events A and B are mutually exclusive if they cannot both occur at the same time.\n\n# Probability of an Event Example\n\n# Number of favorable outcomes for event A\nfavorable_outcomes_A &lt;- length(event_A)\n\n# Total number of possible outcomes\ntotal_outcomes &lt;- length(sample_space)\n\n# Calculate the probability of event A\nprobability_A &lt;- favorable_outcomes_A / total_outcomes\n\n# Display the probability of event A\ncat(\"Probability of Event A (Rolling an even number):\", probability_A, \"\\n\")\n\nProbability of Event A (Rolling an even number): 0.5"
  },
  {
    "objectID": "posts/MLBlog/backup.html#random-variables-and-probability-distributions",
    "href": "posts/MLBlog/backup.html#random-variables-and-probability-distributions",
    "title": "Fundamentals of Machine Learning",
    "section": "",
    "text": "In probability theory, a random variable is a variable whose possible values are outcomes of a random phenomenon. Understanding random variables is crucial in modeling uncertainty and variability in real-world scenarios. Each random variable is associated with a probability distribution, which describes the likelihood of different outcomes.\n\n\nDiscrete Random Variables\n\nExamples: Number of heads in coin flips, count of customers arriving at a store.\nRationale: Takes on a countable number of distinct values.\n\nContinuous Random Variables\n\nExamples: Height of individuals, temperature, stock prices.\nRationale: Takes on an uncountable infinite number of possible values.\n\n\n\n\nProbability Mass Function (PMF)\n\nFor discrete random variables.\nGives the probability of each possible outcome.\n\n\n# Example PMF for a six-sided die\noutcomes &lt;- 1:6\nprobabilities &lt;- rep(1/6, 6)\npmf &lt;- data.frame(Outcome = outcomes, Probability = probabilities)\n\n# Display the pmf\nprint(pmf)\n\n  Outcome Probability\n1       1   0.1666667\n2       2   0.1666667\n3       3   0.1666667\n4       4   0.1666667\n5       5   0.1666667\n6       6   0.1666667\n\n\nProbability Density Function (PDF)\n\nFor continuous random variables.\nDescribes the likelihood of a range of values.\n\n\n# Example PDF for a standard normal distribution\nx &lt;- seq(-3, 3, length.out = 100)\npdf &lt;- data.frame(X = x, Probability = dnorm(x))\n\n# Display the pdf\nprint(pdf)\n\n              X Probability\n1   -3.00000000 0.004431848\n2   -2.93939394 0.005305788\n3   -2.87878788 0.006328776\n4   -2.81818182 0.007521325\n5   -2.75757576 0.008905818\n6   -2.69696970 0.010506499\n7   -2.63636364 0.012349433\n8   -2.57575758 0.014462415\n9   -2.51515152 0.016874830\n10  -2.45454545 0.019617461\n11  -2.39393939 0.022722232\n12  -2.33333333 0.026221889\n13  -2.27272727 0.030149614\n14  -2.21212121 0.034538568\n15  -2.15151515 0.039421369\n16  -2.09090909 0.044829497\n17  -2.03030303 0.050792644\n18  -1.96969697 0.057338005\n19  -1.90909091 0.064489518\n20  -1.84848485 0.072267075\n21  -1.78787879 0.080685709\n22  -1.72727273 0.089754773\n23  -1.66666667 0.099477139\n24  -1.60606061 0.109848419\n25  -1.54545455 0.120856256\n26  -1.48484848 0.132479675\n27  -1.42424242 0.144688550\n28  -1.36363636 0.157443188\n29  -1.30303030 0.170694048\n30  -1.24242424 0.184381641\n31  -1.18181818 0.198436597\n32  -1.12121212 0.212779929\n33  -1.06060606 0.227323506\n34  -1.00000000 0.241970725\n35  -0.93939394 0.256617400\n36  -0.87878788 0.271152848\n37  -0.81818182 0.285461167\n38  -0.75757576 0.299422683\n39  -0.69696970 0.312915556\n40  -0.63636364 0.325817499\n41  -0.57575758 0.338007591\n42  -0.51515152 0.349368138\n43  -0.45454545 0.359786558\n44  -0.39393939 0.369157220\n45  -0.33333333 0.377383228\n46  -0.27272727 0.384378084\n47  -0.21212121 0.390067204\n48  -0.15151515 0.394389234\n49  -0.09090909 0.397297160\n50  -0.03030303 0.398759153\n51   0.03030303 0.398759153\n52   0.09090909 0.397297160\n53   0.15151515 0.394389234\n54   0.21212121 0.390067204\n55   0.27272727 0.384378084\n56   0.33333333 0.377383228\n57   0.39393939 0.369157220\n58   0.45454545 0.359786558\n59   0.51515152 0.349368138\n60   0.57575758 0.338007591\n61   0.63636364 0.325817499\n62   0.69696970 0.312915556\n63   0.75757576 0.299422683\n64   0.81818182 0.285461167\n65   0.87878788 0.271152848\n66   0.93939394 0.256617400\n67   1.00000000 0.241970725\n68   1.06060606 0.227323506\n69   1.12121212 0.212779929\n70   1.18181818 0.198436597\n71   1.24242424 0.184381641\n72   1.30303030 0.170694048\n73   1.36363636 0.157443188\n74   1.42424242 0.144688550\n75   1.48484848 0.132479675\n76   1.54545455 0.120856256\n77   1.60606061 0.109848419\n78   1.66666667 0.099477139\n79   1.72727273 0.089754773\n80   1.78787879 0.080685709\n81   1.84848485 0.072267075\n82   1.90909091 0.064489518\n83   1.96969697 0.057338005\n84   2.03030303 0.050792644\n85   2.09090909 0.044829497\n86   2.15151515 0.039421369\n87   2.21212121 0.034538568\n88   2.27272727 0.030149614\n89   2.33333333 0.026221889\n90   2.39393939 0.022722232\n91   2.45454545 0.019617461\n92   2.51515152 0.016874830\n93   2.57575758 0.014462415\n94   2.63636364 0.012349433\n95   2.69696970 0.010506499\n96   2.75757576 0.008905818\n97   2.81818182 0.007521325\n98   2.87878788 0.006328776\n99   2.93939394 0.005305788\n100  3.00000000 0.004431848\n\n\n\n\n\n\nExpectation (Mean): Represents the average value of a random variable.\n\n\n# Example calculation of expectation for a discrete random variable\nmean_discrete &lt;- sum(outcomes * probabilities)\n\n# Display the mean\ncat(\"mean:\", mean_discrete, \"\\n\")\n\nmean: 3.5 \n\n\n\nVariance: Measures the spread or dispersion of a random variable.\n\n\n# Example calculation of variance for a discrete random variable\nvariance_discrete &lt;- sum((outcomes - mean_discrete)^2 * probabilities)\n\n# Display the variance\ncat(\"variance:\", variance_discrete, \"\\n\")\n\nvariance: 2.916667"
  },
  {
    "objectID": "posts/MLBlog/backup.html#real-world-application",
    "href": "posts/MLBlog/backup.html#real-world-application",
    "title": "Fundamentals of Machine Learning",
    "section": "",
    "text": "Understanding basic probability concepts is crucial in various real-world scenarios, from predicting the weather to making informed decisions in business and finance. By grasping these fundamentals, we can quantify uncertainty and make more informed choices. Random variables and probability distributions are essential in fields like finance (modeling stock prices), healthcare (analyzing patient outcomes), and many others. These concepts form the backbone of statistical modeling and machine learning algorithms."
  },
  {
    "objectID": "posts/MLBlog/backup.html#conclusion",
    "href": "posts/MLBlog/backup.html#conclusion",
    "title": "Fundamentals of Machine Learning",
    "section": "",
    "text": "Probability theory provides a solid foundation for understanding uncertainty and variability in data—a cornerstone for anyone venturing into machine learning. Mastering basic probability concepts sets the stage for a deeper exploration of probability theory. In the next sections, we’ll apply these concepts through practical examples and code implementations."
  },
  {
    "objectID": "posts/MLBlog/backup.html#introduction-1",
    "href": "posts/MLBlog/backup.html#introduction-1",
    "title": "Fundamentals of Machine Learning",
    "section": "Introduction",
    "text": "Introduction\nClustering is a technique in machine learning that involves grouping similar data points together based on certain characteristics. The goal is to uncover inherent structures within the data, making it a form of unsupervised learning. In this section, we’ll explore the basics of clustering, with a focus on one popular algorithm: K-Means."
  },
  {
    "objectID": "posts/MLBlog/backup.html#k-means-clustering-algorithm",
    "href": "posts/MLBlog/backup.html#k-means-clustering-algorithm",
    "title": "Fundamentals of Machine Learning",
    "section": "K-Means Clustering Algorithm",
    "text": "K-Means Clustering Algorithm\nK-Means is a partitioning method that divides a dataset into \\(k\\) distinct, non-overlapping subsets (clusters).\n\nHow it Works\n\nInitialization: Randomly select \\(k\\) data points as initial cluster centroids.\nAssignment: Assign each data point to the cluster whose centroid is closest.\nUpdate: Recalculate the cluster centroids based on the assigned points.\nRepeat: Iterate steps 2 and 3 until convergence (minimal change in cluster assignments).\n\n\n# K-Means clustering example using built-in dataset iris\ndata &lt;- iris[, 1:4]  # Selecting relevant features\nkmeans_result &lt;- kmeans(data, centers = 3)  # Clustering into 3 groups\n\n# Display cluster assignments\ncat(\"Cluster Assignments:\", kmeans_result$cluster, \"\\n\")\n\nCluster Assignments: 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 1 2 2 2 2 2 2 1 1 2 2 2 2 1 2 1 2 1 2 2 1 1 2 2 2 2 2 1 2 2 2 2 1 2 2 2 1 2 2 2 1 2 2 1 \n\n# Display cluster centroids\ncat(\"Cluster Centroids:\\n\", kmeans_result$centers, \"\\n\")\n\nCluster Centroids:\n 5.901613 6.85 5.006 2.748387 3.073684 3.428 4.393548 5.742105 1.462 1.433871 2.071053 0.246 \n\n# Scatter plot with cluster assignments\nplot(data[, c(1, 2)], col = kmeans_result$cluster, main = \"K-Means Clustering\", \n     xlab = \"Sepal Length\", ylab = \"Sepal Width\")\n\n# Plotting cluster centroids\npoints(kmeans_result$centers[, c(1, 2)], col = 1:3, pch = 8, cex = 2)"
  },
  {
    "objectID": "posts/MLBlog/backup.html#real-world-application-1",
    "href": "posts/MLBlog/backup.html#real-world-application-1",
    "title": "Fundamentals of Machine Learning",
    "section": "Real-World Application",
    "text": "Real-World Application\nClustering is widely used in various domains:\n\nCustomer Segmentation: Identifying groups of customers with similar purchasing behaviors.\nImage Segmentation: Grouping pixels with similar attributes in images.\nAnomaly Detection: Identifying unusual patterns in data by treating normal behavior as clusters."
  },
  {
    "objectID": "posts/MLBlog/backup.html#conclusion-1",
    "href": "posts/MLBlog/backup.html#conclusion-1",
    "title": "Fundamentals of Machine Learning",
    "section": "Conclusion",
    "text": "Conclusion\nUnderstanding clustering algorithms like K-Means opens up opportunities to uncover patterns in data, enabling more informed decision-making. In the next sections, we’ll explore additional machine learning concepts, including regression, classification, and anomaly detection."
  },
  {
    "objectID": "posts/MLBlog/backup.html#introduction-2",
    "href": "posts/MLBlog/backup.html#introduction-2",
    "title": "Fundamentals of Machine Learning",
    "section": "Introduction",
    "text": "Introduction\nRegression is a fundamental concept in machine learning that involves modeling the relationship between a dependent variable and one or more independent variables. In this section, we’ll explore linear regression, a widely used regression technique, and nonlinear regression. Nonlinear regression is a powerful technique used in machine learning when the relationship between the dependent variable and the independent variables cannot be adequately modeled with a linear equation."
  },
  {
    "objectID": "posts/MLBlog/backup.html#linear-regression",
    "href": "posts/MLBlog/backup.html#linear-regression",
    "title": "Fundamentals of Machine Learning",
    "section": "Linear Regression",
    "text": "Linear Regression\nLinear regression aims to find the best-fit linear relationship between a dependent variable (\\(Y\\)) and one or more independent variables (\\(X\\)). The goal is to model the underlying pattern in the data, making it a valuable tool for prediction and understanding the relationships between variables.\n\nLinear Regression Model\nModel Equation \\(Y=β0​+β1​X1​+β2​X2​+…+βn​Xn​+ε\\)\n\n\\(Y\\) is the dependent variable.\n\\(X1​,X2​,…,Xn​\\) are the independent variables.\n\\(β0​,β1​,…,βn​\\) are the model coefficients.\n\\(ε\\) is the error term.\n\n\n# Linear regression example using built-in dataset mtcars\nlinear_model &lt;- lm(mpg ~ hp, data = mtcars)  # Predicting mpg based on horsepower\n\n# Display regression summary\nsummary(linear_model)\n\n\nCall:\nlm(formula = mpg ~ hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7121 -2.1122 -0.8854  1.5819  8.2360 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 30.09886    1.63392  18.421  &lt; 2e-16 ***\nhp          -0.06823    0.01012  -6.742 1.79e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 30 degrees of freedom\nMultiple R-squared:  0.6024,    Adjusted R-squared:  0.5892 \nF-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07\n\n# Scatter plot with linear regression line\nplot(mtcars$hp, mtcars$mpg, main = \"Linear Regression\", xlab = \"Horsepower\", ylab = \"Miles Per Gallon\")\n\n# Adding regression line\nabline(linear_model, col = \"red\")"
  },
  {
    "objectID": "posts/MLBlog/backup.html#nonlinear-regression-model",
    "href": "posts/MLBlog/backup.html#nonlinear-regression-model",
    "title": "Fundamentals of Machine Learning",
    "section": "Nonlinear Regression Model",
    "text": "Nonlinear Regression Model\nWhile linear regression assumes a linear relationship between variables, nonlinear regression allows for more complex relationships. The model equation is a nonlinear function of the independent variables, providing greater flexibility in capturing intricate patterns in the data.\n\nNonlinear Regression Model\nModel Equation\n\\(Y=β0​+β1​f1​(X1​)+β2​f2​(X2​)+…+βn​fn​(Xn​)+ε\\)\n\n\\(Y\\) is the dependent variable.\n\\(f1​(X1​),f2​(X2​),…,fn​(Xn​)\\) are nonlinear functions of the independent variables.\n\\(β0​,β1​,…,βn​\\) are the model coefficients.\n\\(ε\\) is the error term.\n\n\n# Nonlinear regression example using built-in dataset mtcars\nnonlinear_model &lt;- nls(mpg ~ a * hp^b, data = mtcars, start = list(a = 1, b = 1))  # Nonlinear model: a * hp^b\n\n# Display regression summary\nsummary(nonlinear_model)\n\n\nFormula: mpg ~ a * hp^b\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \na 272.11686   74.03677   3.675 0.000924 ***\nb  -0.54043    0.05826  -9.277 2.55e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.102 on 30 degrees of freedom\n\nNumber of iterations to convergence: 18 \nAchieved convergence tolerance: 5.914e-07\n\n# Scatter plot with nonlinear regression curve\nplot(mtcars$hp, mtcars$mpg, main = \"Nonlinear Regression\", xlab = \"Horsepower\", ylab = \"Miles Per Gallon\")\n\n# Adding nonlinear regression curve\nlines(mtcars$hp, predict(nonlinear_model), col = \"red\")"
  },
  {
    "objectID": "posts/MLBlog/backup.html#real-world-applications",
    "href": "posts/MLBlog/backup.html#real-world-applications",
    "title": "Fundamentals of Machine Learning",
    "section": "Real-World Applications",
    "text": "Real-World Applications\nLinear regression finds applications in various fields:\n\nEconomics: Predicting economic indicators based on various factors.\nHealthcare: Modeling the relationship between patient characteristics and health outcomes.\nFinance: Predicting stock prices based on historical data.\n\nNonlinear regression is applicable in scenarios where relationships are better represented by curves or other nonlinear patterns:\n\nPharmacokinetics: Modeling drug concentration over time in the body.\nBiology: Modeling population growth, enzyme kinetics, etc.\nPhysics: Modeling complex physical phenomena."
  },
  {
    "objectID": "posts/MLBlog/backup.html#conclusion-2",
    "href": "posts/MLBlog/backup.html#conclusion-2",
    "title": "Fundamentals of Machine Learning",
    "section": "Conclusion",
    "text": "Conclusion\nUnderstanding linear regression allows us to make predictions and gain insights into the relationships between variables. Nonlinear regression expands our modeling capabilities, allowing us to capture more intricate relationships in the data. In the upcoming sections, we’ll explore classification, and anomaly detection."
  },
  {
    "objectID": "posts/MLBlog/backup.html#introduction-3",
    "href": "posts/MLBlog/backup.html#introduction-3",
    "title": "Fundamentals of Machine Learning",
    "section": "Introduction",
    "text": "Introduction\nClassification is a type of supervised machine learning where the goal is to predict the categorical class labels of new instances based on past observations. Classification involves training a model to learn the mapping between input features and predefined categories or classes. The trained model can then be used to predict the class labels of new, unseen instances. In this section, we’ll explore the basics of classification, with a focus on one of the most widely used algorithms: Decision Trees."
  },
  {
    "objectID": "posts/MLBlog/backup.html#decision-trees-for-classification",
    "href": "posts/MLBlog/backup.html#decision-trees-for-classification",
    "title": "Fundamentals of Machine Learning",
    "section": "Decision Trees for Classification",
    "text": "Decision Trees for Classification\nA decision tree is a tree-like model where each internal node represents a decision based on a feature, each branch represents the outcome of the decision, and each leaf node represents the class label.\n\nHow it Works\n\nFeature Selection: Choose the feature that best splits the data into classes (based on criteria like Gini impurity or information gain).\nSplitting: Create a branch for each possible value of the selected feature.\nRepeat: Recursively apply steps 1 and 2 to each subset of data until a stopping criterion is met.\nLeaf Nodes: Assign a class label to each leaf node.\n\n\n# Decision tree example using built-in dataset iris\nlibrary(rpart)\n\n# Creating a decision tree model\ntree_model &lt;- rpart(Species ~ ., data = iris, method = \"class\")\n\n# Displaying the decision tree plot\nplot(tree_model)\ntext(tree_model)"
  },
  {
    "objectID": "posts/MLBlog/backup.html#real-world-application-2",
    "href": "posts/MLBlog/backup.html#real-world-application-2",
    "title": "Fundamentals of Machine Learning",
    "section": "Real-World Application",
    "text": "Real-World Application\nDecision trees find applications in various domains:\n\nMedical Diagnosis: Predicting diseases based on patient symptoms.\nCustomer Churn Prediction: Identifying customers likely to churn from a service.\nCredit Scoring: Assessing credit risk of loan applicants."
  },
  {
    "objectID": "posts/MLBlog/backup.html#conclusion-3",
    "href": "posts/MLBlog/backup.html#conclusion-3",
    "title": "Fundamentals of Machine Learning",
    "section": "Conclusion",
    "text": "Conclusion\nClassification is a powerful tool for making predictions and categorizing data into distinct classes. In the last sections, we’ll explore anomaly detection, a technique used to identify unusual patterns in data."
  },
  {
    "objectID": "posts/MLBlog/backup.html#introduction-4",
    "href": "posts/MLBlog/backup.html#introduction-4",
    "title": "Fundamentals of Machine Learning",
    "section": "Introduction",
    "text": "Introduction\nAnomaly detection, also known as outlier detection, is a machine learning technique used to identify patterns in data that deviate significantly from the norm. In this section, we’ll explore the basics of anomaly detection, with a focus on a common method: Z-score-based outlier detection."
  },
  {
    "objectID": "posts/MLBlog/backup.html#outlier-detection-methods",
    "href": "posts/MLBlog/backup.html#outlier-detection-methods",
    "title": "Fundamentals of Machine Learning",
    "section": "Outlier Detection Methods",
    "text": "Outlier Detection Methods\nZ-Score Method\n\nCalculates the Z-score for each data point, representing how many standard deviations it is from the mean.\nPoints with a Z-score above a certain threshold are considered outliers.\n\nExample: Outlier Detection using Z-Score\n\n# Outlier detection using Z-score\ndata &lt;- c(2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31)\n\n# Calculate Z-scores\nz_scores &lt;- (data - mean(data)) / sd(data)\n\n# Set Z-score threshold for outliers\nz_threshold &lt;- 2\n\n# Identify outliers\noutliers &lt;- abs(z_scores) &gt; z_threshold\n\n# Display identified outliers\ncat(\"Outliers:\", data[outliers], \"\\n\")\n\nOutliers:  \n\n# Scatter plot with highlighted outliers\nplot(1:length(data), data, pch = ifelse(outliers, 19, 1), main = \"Scatter Plot with Outliers\", xlab = \"Index\", ylab = \"Values\")"
  },
  {
    "objectID": "posts/MLBlog/backup.html#real-world-application-3",
    "href": "posts/MLBlog/backup.html#real-world-application-3",
    "title": "Fundamentals of Machine Learning",
    "section": "Real-World Application",
    "text": "Real-World Application\nAnomaly detection is applied in various fields:\n\nFraud Detection: Identifying unusual patterns in financial transactions.\nNetwork Security: Detecting unusual behavior in network traffic.\nManufacturing Quality Control: Identifying defective products."
  },
  {
    "objectID": "posts/MLBlog/backup.html#conclusion-4",
    "href": "posts/MLBlog/backup.html#conclusion-4",
    "title": "Fundamentals of Machine Learning",
    "section": "Conclusion",
    "text": "Conclusion\nAnomaly detection is crucial for identifying unusual patterns that may indicate errors or potential issues."
  },
  {
    "objectID": "Post1b.html#the-result-of-a-coin-flip-using-a-bernoulli-distribution-in-r-you-can-create-a-bar-plot-that-represents-the-probability-of-getting-heads-and-tails.-the-bar-heights-will-correspond-to-the-probabilities-of-success-heads-and-failure-tails.",
    "href": "Post1b.html#the-result-of-a-coin-flip-using-a-bernoulli-distribution-in-r-you-can-create-a-bar-plot-that-represents-the-probability-of-getting-heads-and-tails.-the-bar-heights-will-correspond-to-the-probabilities-of-success-heads-and-failure-tails.",
    "title": "Basic Probability Concepts",
    "section": "The result of a coin flip using a Bernoulli distribution in R, you can create a bar plot that represents the probability of getting Heads and Tails. The bar heights will correspond to the probabilities of success (Heads) and failure (Tails).",
    "text": "The result of a coin flip using a Bernoulli distribution in R, you can create a bar plot that represents the probability of getting Heads and Tails. The bar heights will correspond to the probabilities of success (Heads) and failure (Tails).\n\n\nCode\n# Set the probability of getting Heads\np_heads &lt;- 0.5\n\n# Calculate the probability of getting Tails\np_tails &lt;- 1 - p_heads\n\n# Create a bar plot\nbarplot(c(p_heads, p_tails), names.arg = c(\"Heads\", \"Tails\"),\n        col = c(\"blue\", \"red\"), ylim = c(0, 1),\n        main = \"Coin Flip Probability\",\n        xlab = \"Outcome\", ylab = \"Probability\")\n\n# Add probability labels to the bars\ntext(1:2, c(p_heads, p_tails), labels = c(p_heads, p_tails), pos = 3, col = \"black\")"
  }
]